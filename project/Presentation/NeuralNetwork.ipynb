{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.html.widgets import interact\n",
    "\n",
    "from sklearn.datasets import load_digits\n",
    "digits = load_digits()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"This part defines the sigmoid functions, which ae used to calculate the\n",
    "output of each neuron. At large positive and negative x, it approximates the step\n",
    "function, but curves near zero.\"\"\"\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1/(1 + np.exp(-x))\n",
    "\n",
    "sigmoid_v = np.vectorize(sigmoid)\n",
    "\n",
    "def sigmoidprime(x):\n",
    "    return sigmoid(x) * (1 - sigmoid(x))\n",
    "\n",
    "sigmoidprime_v = np.vectorize(sigmoidprime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.3, 1.3)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAD7CAYAAABpJS8eAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n+GZmOeHANzPLCQe+mVlOOPDNzHLi/wP2kZhWeFJ10gAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f2639b65828>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = np.linspace(-15, 15, 100)\n",
    "y = sigmoid(x)\n",
    "plt.plot(x, y)\n",
    "plt.ylim(-0.3, 1.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"This section created random initial weights and biases. It also takes the\n",
    "data and answers from the source, and converts the answers into 10-d vectors\n",
    "\n",
    "ex. 6 --> [0, 0, 0, 0, 0, 0, 1, 0, 0, 0]\"\"\"\n",
    "\n",
    "size = [64, 20, 10]\n",
    "\n",
    "weights = []\n",
    "for n in range(1, len(size)):\n",
    "    weights.append(np.random.rand(size[n], size[n-1]) * 2 - 1)\n",
    "\n",
    "biases = []\n",
    "for n in range(1, len(size)):\n",
    "    biases.append(np.random.rand(size[n]) * 2 - 1)\n",
    "\n",
    "trainingdata = digits.data[0:1200]\n",
    "traininganswers = digits.target[0:1200]\n",
    "lc = 0.02\n",
    "\n",
    "#convert the integer answers into a 10-dimension array\n",
    "traininganswervectors = np.zeros((1796,10))\n",
    "for n in range(1796):\n",
    "    traininganswervectors[n][digits.target[n]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"This function calculates the output of the network based on the weights and\n",
    "biases, and uses the sigmoid function.\"\"\"\n",
    "def feedforward(weights, biases, a):\n",
    "    b = []\n",
    "    #first element is inputs \"a\"\n",
    "    b.append(a)\n",
    "    for n in range(1, len(size)):\n",
    "        #all other elements depend on the number of neurons\n",
    "        b.append(np.zeros(size[n]))\n",
    "        for n2 in range(0, size[n]):\n",
    "            b[n][n2] = sigmoid_v(np.dot(weights[n-1][n2], b[n-1]) + biases[n-1][n2])\n",
    "      \n",
    "    return b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([  0.,   0.,   5.,  13.,   9.,   1.,   0.,   0.,   0.,   0.,  13.,\n",
       "         15.,  10.,  15.,   5.,   0.,   0.,   3.,  15.,   2.,   0.,  11.,\n",
       "          8.,   0.,   0.,   4.,  12.,   0.,   0.,   8.,   8.,   0.,   0.,\n",
       "          5.,   8.,   0.,   0.,   9.,   8.,   0.,   0.,   4.,  11.,   0.,\n",
       "          1.,  12.,   7.,   0.,   0.,   2.,  14.,   5.,  10.,  12.,   0.,\n",
       "          0.,   0.,   0.,   6.,  13.,  10.,   0.,   0.,   0.]),\n",
       " array([  3.21682303e-06,   7.51067561e-02,   4.85302615e-02,\n",
       "          9.44295088e-01,   1.00000000e+00,   5.60922030e-19,\n",
       "          9.99995209e-01,   2.80763966e-17,   2.17687836e-21,\n",
       "          1.00000000e+00,   1.00000000e+00,   9.98164651e-01,\n",
       "          2.55007164e-16,   9.78797916e-06,   1.00000000e+00,\n",
       "          2.91803266e-19,   1.00000000e+00,   9.99998105e-01,\n",
       "          1.00000000e+00,   3.79032818e-23]),\n",
       " array([ 0.93938143,  0.85755443,  0.90042992,  0.54905622,  0.49554056,\n",
       "         0.64940774,  0.21117831,  0.02616924,  0.57829814,  0.0189328 ])]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feedforward(weights, biases, trainingdata[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"This function just calculates the difference between the answer the network\n",
    "provides and the given answer for an input for each element of the answer\"\"\"\n",
    "\n",
    "def costderivative(output, answers):\n",
    "    return (output - answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"This function is used to find the 'minimum' of the cost derivative. It selects\n",
    "a 'minibatch' of random inputs from the training set to approximate the behavior\n",
    "of the whole set.\"\"\"\n",
    "\n",
    "def gradient_descent(weights, biases, inputs, answers, batchsize, lc, epochs):\n",
    "    for n in range(epochs):\n",
    "        #pick random locations for input/result data\n",
    "        locations = np.random.randint(0, len(inputs), batchsize)\n",
    "        minibatch = []\n",
    "        #create tuples (inputs, result) based on random locations\n",
    "        for n2 in range(batchsize):\n",
    "            minibatch.append((inputs[locations[n2]], answers[locations[n2]]))\n",
    "        for n3 in range(batchsize):\n",
    "            weights, biases = train(weights, biases, minibatch, lc)\n",
    "        \n",
    "        \n",
    "        results = []\n",
    "        for n4 in range(len(trainingdata)):\n",
    "            results.append(feedforward(weights, biases, inputs[n4])[-1])\n",
    "            \n",
    "        accresult = accuracy(inputs, results, answers)\n",
    "        print(\"Epoch \", n, \" : \", accresult)\n",
    "        \n",
    "    return weights, biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"This function adjusts the weights and biases based on the results of the\n",
    "backpropagation function\"\"\"\n",
    "\n",
    "def train(weights, biases, minibatch, lc):\n",
    "    #set the nabla functions to be the functions themselves initially, same size\n",
    "    nb = [np.zeros(b.shape) for b in biases]\n",
    "    nw = [np.zeros(w.shape) for w in weights]\n",
    "    #largely taken from Michael Nielsen's implementation\n",
    "    for i, r in minibatch:\n",
    "        dnb, dnw = backprop(weights, biases, i, r)\n",
    "        nb = [a+b for a, b in zip(nb, dnb)]\n",
    "        nw = [a+b for a, b in zip(nw, dnw)]\n",
    "    \n",
    "    weights = [w-(lc/len(minibatch))*n_w for w, n_w in zip(weights, nw)]\n",
    "    biases = [b-(lc/len(minibatch))*n_b for b, n_b in zip(biases, nb)]\n",
    "    return weights, biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"This function is the most complex, and likely where I made an error. It \n",
    "calculates the gradient of the change for the output, then calculates it for\n",
    "each step before it (hence BACKpropagation).\"\"\"\n",
    "\n",
    "def backprop(weights, biases, inputs, answers):\n",
    "    #set the nabla functions to be the same size as functions\n",
    "    nb = [np.zeros(b.shape) for b in biases]\n",
    "    nw = [np.zeros(w.shape) for w in weights]\n",
    "    a = inputs\n",
    "    alist = [inputs]\n",
    "    zlist = []\n",
    "    #from feedforward\n",
    "    for n in range(1, len(size)):\n",
    "        #all other elements depend on the number of neurons\n",
    "        zlist.append(np.zeros(size[n]))\n",
    "        alist.append(np.zeros(size[n]))\n",
    "        for n2 in range(0, size[n]):\n",
    "            zlist[n-1][n2] = np.dot(weights[n-1][n2], alist[n-1]) + biases[n-1][n2]\n",
    "            alist[n][n2] = sigmoid_v(zlist[n-1][n2])\n",
    "    \n",
    "    delta = costderivative(alist[-1], answers) * sigmoidprime_v(zlist[-1])\n",
    "    nb[-1] = delta\n",
    "    #different from MN, alist[-2] not same size as delta?\n",
    "    nw[-1] = np.dot(delta, alist[-1].transpose())\n",
    "    \n",
    "    for n in range(2, len(size)):\n",
    "        delta = np.dot(weights[-n+1].transpose(), delta) * sigmoidprime_v(zlist[-n])\n",
    "        nb[-n] = delta\n",
    "        #same here\n",
    "        nw[-n] = np.dot(delta, alist[-n].transpose())\n",
    "    \n",
    "    return nb, nw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"This function is just used to test the accuracy of each epoch. It converts the\n",
    "outputs into a vector like the answers, taking the largest value to be the '1'\n",
    "\n",
    "ex [0.21, 0.06, 0.134, 0.952, 0.558, 0.031, 0.511, 0.105, 0.216, 0.041] -->\n",
    "    [0, 0, 0, 1, 0, 0, 0, 0, 0, 0] == 3\"\"\"\n",
    "\n",
    "def accuracy(inputs, results, answers):\n",
    "    correct = 0\n",
    "    binresults = results\n",
    "    for n in range(0, len(results)):\n",
    "        #converts the output into a binary y/n for each digit\n",
    "        for n2 in range(len(results[n])):\n",
    "            if results[n][n2] == np.amax(results[n]):\n",
    "                binresults[n][n2] = 1\n",
    "            else:\n",
    "                binresults[n][n2] = 0\n",
    "        \n",
    "        if np.array_equal(answers[n], binresults[n]):\n",
    "            correct += 1\n",
    "    return correct / len(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "size = [64, 20, 10]\n",
    "\n",
    "weights = []\n",
    "for n in range(1, len(size)):\n",
    "    weights.append(np.random.rand(size[n], size[n-1]) * 2 - 1)\n",
    "\n",
    "biases = []\n",
    "for n in range(1, len(size)):\n",
    "    biases.append(np.random.rand(size[n]) * 2 - 1)\n",
    "\n",
    "trainingdata = digits.data[0:500]\n",
    "traininganswers = digits.target[0:500]\n",
    "\n",
    "traininganswervectors = np.zeros((500,10))\n",
    "for n in range(500):\n",
    "    traininganswervectors[n][digits.target[n]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  0  :  0.122\n",
      "Epoch  1  :  0.122\n",
      "Epoch  2  :  0.134\n",
      "Epoch  3  :  0.146\n",
      "Epoch  4  :  0.11\n",
      "Epoch  5  :  0.136\n",
      "Epoch  6  :  0.136\n",
      "Epoch  7  :  0.124\n",
      "Epoch  8  :  0.116\n",
      "Epoch  9  :  0.136\n",
      "Epoch  10  :  0.124\n",
      "Epoch  11  :  0.104\n",
      "Epoch  12  :  0.118\n",
      "Epoch  13  :  0.104\n",
      "Epoch  14  :  0.108\n",
      "Epoch  15  :  0.106\n",
      "Epoch  16  :  0.102\n",
      "Epoch  17  :  0.092\n",
      "Epoch  18  :  0.09\n",
      "Epoch  19  :  0.09\n",
      "Epoch  20  :  0.09\n",
      "Epoch  21  :  0.096\n",
      "Epoch  22  :  0.098\n",
      "Epoch  23  :  0.1\n",
      "Epoch  24  :  0.096\n",
      "Epoch  25  :  0.108\n",
      "Epoch  26  :  0.104\n",
      "Epoch  27  :  0.1\n",
      "Epoch  28  :  0.1\n",
      "Epoch  29  :  0.1\n",
      "Epoch  30  :  0.108\n",
      "Epoch  31  :  0.098\n",
      "Epoch  32  :  0.124\n",
      "Epoch  33  :  0.1\n",
      "Epoch  34  :  0.098\n",
      "Epoch  35  :  0.092\n",
      "Epoch  36  :  0.106\n",
      "Epoch  37  :  0.104\n",
      "Epoch  38  :  0.104\n",
      "Epoch  39  :  0.1\n",
      "Epoch  40  :  0.11\n",
      "Epoch  41  :  0.108\n",
      "Epoch  42  :  0.102\n",
      "Epoch  43  :  0.1\n",
      "Epoch  44  :  0.092\n",
      "Epoch  45  :  0.096\n",
      "Epoch  46  :  0.104\n",
      "Epoch  47  :  0.102\n",
      "Epoch  48  :  0.11\n",
      "Epoch  49  :  0.126\n",
      "Epoch  50  :  0.112\n",
      "Epoch  51  :  0.11\n",
      "Epoch  52  :  0.112\n",
      "Epoch  53  :  0.12\n",
      "Epoch  54  :  0.124\n",
      "Epoch  55  :  0.102\n",
      "Epoch  56  :  0.094\n",
      "Epoch  57  :  0.104\n",
      "Epoch  58  :  0.106\n",
      "Epoch  59  :  0.1\n",
      "Epoch  60  :  0.104\n",
      "Epoch  61  :  0.084\n",
      "Epoch  62  :  0.068\n",
      "Epoch  63  :  0.066\n",
      "Epoch  64  :  0.07\n",
      "Epoch  65  :  0.078\n",
      "Epoch  66  :  0.1\n",
      "Epoch  67  :  0.098\n",
      "Epoch  68  :  0.098\n",
      "Epoch  69  :  0.08\n",
      "Epoch  70  :  0.08\n",
      "Epoch  71  :  0.082\n",
      "Epoch  72  :  0.08\n",
      "Epoch  73  :  0.072\n",
      "Epoch  74  :  0.072\n",
      "Epoch  75  :  0.072\n",
      "Epoch  76  :  0.07\n",
      "Epoch  77  :  0.07\n",
      "Epoch  78  :  0.072\n",
      "Epoch  79  :  0.082\n",
      "Epoch  80  :  0.078\n",
      "Epoch  81  :  0.08\n",
      "Epoch  82  :  0.092\n",
      "Epoch  83  :  0.11\n",
      "Epoch  84  :  0.084\n",
      "Epoch  85  :  0.068\n",
      "Epoch  86  :  0.062\n",
      "Epoch  87  :  0.066\n",
      "Epoch  88  :  0.102\n",
      "Epoch  89  :  0.102\n",
      "Epoch  90  :  0.1\n",
      "Epoch  91  :  0.1\n",
      "Epoch  92  :  0.1\n",
      "Epoch  93  :  0.1\n",
      "Epoch  94  :  0.098\n",
      "Epoch  95  :  0.066\n",
      "Epoch  96  :  0.072\n",
      "Epoch  97  :  0.048\n",
      "Epoch  98  :  0.076\n",
      "Epoch  99  :  0.076\n"
     ]
    }
   ],
   "source": [
    "final_weights, final_biases = gradient_descent(weights, biases, trainingdata,\n",
    "                                              traininganswervectors, 5, 1, 100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
