{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ":0: FutureWarning: IPython widgets are experimental and may change in the future.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.html.widgets import interact\n",
    "\n",
    "from sklearn.datasets import load_digits\n",
    "digits = load_digits()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1/(1 + np.exp(-x))\n",
    "\n",
    "sigmoid_v = np.vectorize(sigmoid)\n",
    "\n",
    "def sigmoidprime(x):\n",
    "    return sigmoid(x) * (1 - sigmoid(x))\n",
    "\n",
    "sigmoidprime_v = np.vectorize(sigmoidprime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "size = [64, 20, 10]\n",
    "\n",
    "weights = []\n",
    "for n in range(1, len(size)):\n",
    "    weights.append(np.random.rand(size[n], size[n-1]) * 2 - 1)\n",
    "\n",
    "biases = []\n",
    "for n in range(1, len(size)):\n",
    "    biases.append(np.random.rand(size[n]) * 2 - 1)\n",
    "\n",
    "trainingdata = digits.data[0:1200]\n",
    "traininganswers = digits.target[0:1200]\n",
    "lc = 0.02\n",
    "\n",
    "#convert the integer answers into a 10-dimension array\n",
    "traininganswervectors = np.zeros((1796,10))\n",
    "for n in range(1796):\n",
    "    traininganswervectors[n][digits.target[n]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def feedforward(weights, biases, a):\n",
    "    b = []\n",
    "    #first element is inputs \"a\"\n",
    "    b.append(a)\n",
    "    for n in range(1, len(size)):\n",
    "        #all other elements depend on the number of neurons\n",
    "        b.append(np.zeros(size[n]))\n",
    "        for n2 in range(0, size[n]):\n",
    "            b[n][n2] = sigmoid_v(np.dot(weights[n-1][n2], b[n-1]) + biases[n-1][n2])\n",
    "      \n",
    "    return b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.49907837  0.76151195  0.0382227   0.17966434  0.52998773  0.68615384\n",
      "  0.27139576  0.64373806  0.50529917  0.81352014]\n",
      "[ 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'costderivative' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-e5821dc7ffd5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopt\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraininganswervectors\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcostderivative\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopt\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraininganswervectors\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'costderivative' is not defined"
     ]
    }
   ],
   "source": [
    "opt = feedforward(weights, biases, trainingdata[0])\n",
    "print(opt[-1])\n",
    "print(traininganswervectors[0])\n",
    "print(costderivative(opt[-1], traininganswervectors[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def gradient_descent(weights, biases, inputs, answers, batchsize, lc, epochs):\n",
    "    for n in range(epochs):\n",
    "        #pick random locations for input/result data\n",
    "        locations = np.random.randint(0, len(inputs), batchsize)\n",
    "        minibatch = []\n",
    "        #create tuples (inputs, result) based on random locations\n",
    "        for n2 in range(batchsize):\n",
    "            minibatch.append((inputs[locations[n2]], answers[locations[n2]]))\n",
    "        for n3 in range(batchsize):\n",
    "            weights, biases = train(weights, biases, minibatch, lc)\n",
    "        \n",
    "        \n",
    "        results = []\n",
    "        for n4 in range(len(trainingdata)):\n",
    "            results.append(feedforward(weights, biases, inputs[n4])[-1])\n",
    "            \n",
    "        accresult = accuracy(inputs, results, answers)\n",
    "        print(\"Epoch \", n, \" : \", accresult)\n",
    "        \n",
    "    return weights, biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train(weights, biases, minibatch, lc):\n",
    "    #set the nabla functions to be the functions themselves initially, same size\n",
    "    nb = [np.zeros(b.shape) for b in biases]\n",
    "    nw = [np.zeros(w.shape) for w in weights]\n",
    "    #largely taken from Michael Nielsen's implementation\n",
    "    for i, r in minibatch:\n",
    "        dnb, dnw = backprop(weights, biases, i, r)\n",
    "        nb = [a+b for a, b in zip(nb, dnb)]\n",
    "        nw = [a+b for a, b in zip(nw, dnw)]\n",
    "    \n",
    "    weights = [w-(lc/len(minibatch))*n_w for w, n_w in zip(weights, nw)]\n",
    "    biases = [b-(lc/len(minibatch))*n_b for b, n_b in zip(biases, nb)]\n",
    "    return weights, biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def backprop(weights, biases, inputs, answers):\n",
    "    #set the nabla functions to be the same size as functions\n",
    "    nb = [np.zeros(b.shape) for b in biases]\n",
    "    nw = [np.zeros(w.shape) for w in weights]\n",
    "    a = inputs\n",
    "    alist = [inputs]\n",
    "    zlist = []\n",
    "    #from feedforward\n",
    "    for n in range(1, len(size)):\n",
    "        #all other elements depend on the number of neurons\n",
    "        zlist.append(np.zeros(size[n]))\n",
    "        alist.append(np.zeros(size[n]))\n",
    "        for n2 in range(0, size[n]):\n",
    "            zlist[n-1][n2] = np.dot(weights[n-1][n2], alist[n-1]) + biases[n-1][n2]\n",
    "            alist[n][n2] = sigmoid_v(zlist[n-1][n2])\n",
    "    \n",
    "    delta = costderivative(alist[-1], answers) * sigmoidprime_v(zlist[-1])\n",
    "    nb[-1] = delta\n",
    "    print(\"delta\", delta)\n",
    "    print(\"alist\", alist)\n",
    "    #different from MN, alist[-2] not same size as delta?\n",
    "    nw[-1] = np.dot(delta, alist[-2].transpose())\n",
    "    \n",
    "    for n in range(2, len(size)):\n",
    "        delta = np.dot(weights[-n+1].transpose(), delta) * sigmoidprime_v(zlist[-n])\n",
    "        nb[-n] = delta\n",
    "        #same here\n",
    "        nw[-n] = np.dot(delta, alist[-n-1].transpose())\n",
    "    \n",
    "    return nb, nw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def costderivative(output, answers):\n",
    "    return (output - answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def accuracy(inputs, results, answers):\n",
    "    correct = 0\n",
    "    binresults = results\n",
    "    for n in range(0, len(results)):\n",
    "        #converts the output into a binary y/n for each digit\n",
    "        for n2 in range(len(results[n])):\n",
    "            if results[n][n2] == np.amax(results[n]):\n",
    "                binresults[n][n2] = 1\n",
    "            else:\n",
    "                binresults[n][n2] = 0\n",
    "        \n",
    "        if np.array_equal(answers[n], binresults[n]):\n",
    "            correct += 1\n",
    "    return correct / len(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "size = [64, 20, 10]\n",
    "\n",
    "weights = []\n",
    "for n in range(1, len(size)):\n",
    "    weights.append(np.random.rand(size[n], size[n-1]) * 2 - 1)\n",
    "\n",
    "biases = []\n",
    "for n in range(1, len(size)):\n",
    "    biases.append(np.random.rand(size[n]) * 2 - 1)\n",
    "\n",
    "trainingdata = digits.data[0:500]\n",
    "traininganswers = digits.target[0:500]\n",
    "\n",
    "traininganswervectors = np.zeros((500,10))\n",
    "for n in range(500):\n",
    "    traininganswervectors[n][digits.target[n]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "delta [ 0.05565875  0.14136822  0.14797691  0.03152321  0.11198302  0.11895867\n",
      "  0.13136808  0.11225746 -0.12030967  0.0020779 ]\n",
      "alist [array([  0.,   0.,   7.,  13.,  15.,   5.,   0.,   0.,   0.,   0.,   8.,\n",
      "        16.,  16.,  12.,   0.,   0.,   0.,   0.,   7.,  16.,  15.,   3.,\n",
      "         0.,   0.,   0.,   0.,   6.,  16.,   5.,   0.,   0.,   0.,   0.,\n",
      "         0.,   5.,  16.,   2.,   0.,   0.,   0.,   0.,   0.,   8.,  16.,\n",
      "         6.,   0.,   0.,   0.,   0.,   0.,  12.,  12.,  13.,   0.,   0.,\n",
      "         0.,   0.,   0.,   5.,  13.,  10.,   0.,   0.,   0.]), array([  9.97221876e-01,   1.00000000e+00,   9.40062072e-01,\n",
      "         1.00000000e+00,   1.13953170e-03,   2.67238421e-15,\n",
      "         2.82055515e-17,   9.98064586e-01,   4.15846773e-01,\n",
      "         7.81768934e-09,   1.66491768e-03,   9.50514785e-02,\n",
      "         8.34626093e-13,   1.00000000e+00,   1.00000000e+00,\n",
      "         1.14756774e-06,   9.99388897e-01,   9.35530420e-16,\n",
      "         9.99842734e-01,   3.42241052e-09]), array([ 0.93654325,  0.74592588,  0.65349386,  0.96623516,  0.84207515,\n",
      "        0.82538401,  0.78894421,  0.45302801,  0.51812791,  0.04668686])]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "shapes (10,) and (20,) not aligned: 10 (dim 0) != 20 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-28-659cabab3c73>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m final_weights, final_biases = gradient_descent(weights, biases, trainingdata,\n\u001b[1;32m----> 2\u001b[1;33m                                               traininganswervectors, 5, 1, 100)\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfinal_weights\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-6-1fc706aefe9d>\u001b[0m in \u001b[0;36mgradient_descent\u001b[1;34m(weights, biases, inputs, answers, batchsize, lc, epochs)\u001b[0m\n\u001b[0;32m      8\u001b[0m             \u001b[0mminibatch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlocations\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mn2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0manswers\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlocations\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mn2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mn3\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatchsize\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m             \u001b[0mweights\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbiases\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbiases\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mminibatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-7-9425d8ff6f22>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(weights, biases, minibatch, lc)\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;31m#largely taken from Michael Nielsen's implementation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mr\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mminibatch\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m         \u001b[0mdnb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdnw\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbackprop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbiases\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m         \u001b[0mnb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mb\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdnb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[0mnw\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mb\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdnw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-24-8a92b8ee7a47>\u001b[0m in \u001b[0;36mbackprop\u001b[1;34m(weights, biases, inputs, answers)\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"alist\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[1;31m#different from MN, alist[-2] not same size as delta?\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m     \u001b[0mnw\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdelta\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malist\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mn\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: shapes (10,) and (20,) not aligned: 10 (dim 0) != 20 (dim 0)"
     ]
    }
   ],
   "source": [
    "final_weights, final_biases = gradient_descent(weights, biases, trainingdata,\n",
    "                                              traininganswervectors, 5, 1, 100)\n",
    "\n",
    "print(final_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
